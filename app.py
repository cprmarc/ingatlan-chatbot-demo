import os
import streamlit as st
from langchain_community.document_loaders import TextLoader, WebBaseLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain.chains import RetrievalQA
from bs4 import BeautifulSoup

# üìÅ Be√°ll√≠t√°sok
DATA_DIR = "data"
INDEX_FILE = "faiss_index"

# üß† OpenAI API-kulcs
openai_api_key = st.secrets.get("OPENAI_API_KEY") or os.getenv("OPENAI_API_KEY")
if not openai_api_key:
    st.error("K√©rlek, √°ll√≠ts be egy OpenAI API-kulcsot a .streamlit/secrets.toml f√°jlban vagy a k√∂rnyezeti v√°ltoz√≥k k√∂z√∂tt.")
    st.stop()

# üßæ URL-ek, amiket be akarunk t√∂lteni
URLS = [
    "https://ingatlan.com/tanacsok/lakasvasarlas",
    "https://ingatlan.com/tanacsok/energetikai-tanusitvany"
]

# üßπ HTML sz√∂veg kiszed√©se
def get_clean_text_from_html(content):
    soup = BeautifulSoup(content, "html.parser")
    for script in soup(["script", "style"]):
        script.extract()
    return soup.get_text(separator=" ", strip=True)

# üß† Tud√°sb√°zis bet√∂lt√©se
def load_data():
    documents = []

    # 1Ô∏è‚É£ TXT f√°jlok
    if os.path.exists(DATA_DIR):
        for filename in os.listdir(DATA_DIR):
            if filename.endswith(".txt"):
                loader = TextLoader(os.path.join(DATA_DIR, filename), encoding='utf-8')
                documents.extend(loader.load())

    # 2Ô∏è‚É£ Webes forr√°sok
    for url in URLS:
        loader = WebBaseLoader(url)
        raw_docs = loader.load()
        for doc in raw_docs:
            doc.page_content = get_clean_text_from_html(doc.page_content)
        documents.extend(raw_docs)

    return documents

# üîÑ Sz√∂veg feldarabol√°sa
def split_documents(documents):
    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    return splitter.split_documents(documents)

# üì¶ Embedding √©s index k√©sz√≠t√©se
def create_or_load_index(documents):
    i
