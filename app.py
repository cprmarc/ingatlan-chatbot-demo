import streamlit as st
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.text_splitter import CharacterTextSplitter
from langchain.docstore.document import Document
from langchain.chains.question_answering import load_qa_chain
from langchain.chat_models import ChatOpenAI
import os
import requests
from bs4 import BeautifulSoup

# SzÃ¶vegtÃ¶rdelÅ‘ beÃ¡llÃ­tÃ¡sa
text_splitter = CharacterTextSplitter(separator="\n", chunk_size=300, chunk_overlap=30)
all_docs = []

# ğŸŒ Online anyagok URL-jei
url_list = [
    "https://tudastar.ingatlan.com/tippek/az-ingatlanvasarlas-menete/",
    "https://tudastar.ingatlan.com/tippek/tulajdonjog-fenntartashoz-kapcsolodo-vevoi-jog/",
    "https://tudastar.ingatlan.com/tippek/birtokbaadasi-jegyzokonyv-mire-valo-miert-jo-hogyan-toltsd-ki/",
    "https://bankmonitor.hu/lakashitel-igenyles",
]

def scrape_url(url):
    try:
        response = requests.get(url)
        soup = BeautifulSoup(response.text, "html.parser")
        paragraphs = soup.find_all("p")
        content = "\n".join(p.get_text() for p in paragraphs)
        return content
    except Exception as e:
        st.warning(f"Nem sikerÃ¼lt lekÃ©rni az URL-t: {url}. Hiba: {e}")
        return ""

# Online cikkek feldolgozÃ¡sa
for url in url_list:
    text = scrape_url(url)
    if text:
        chunks = text_splitter.split_text(text)
        all_docs.extend([Document(page_content=chunk) for chunk in chunks])

# ğŸ“ FAISS index mappÃ¡k
faiss_index_path = "faiss_index"

# Embedding pÃ©ldÃ¡ny
embedding = OpenAIEmbeddings()
vectorstore = None

# Vektor adatbÃ¡zis betÃ¶ltÃ©se vagy lÃ©trehozÃ¡sa
if os.path.exists(faiss_index_path):
    try:
        vectorstore = FAISS.load_local(faiss_index_path, embedding)
        st.success("Index betÃ¶ltve cache-bÅ‘l!")
    except Exception as e:
        st.warning(f"Cache betÃ¶ltÃ©se sikertelen, ÃºjragenerÃ¡lom az indexet. Hiba: {e}")
        vectorstore = None

if vectorstore is None:
    if not all_docs:
        st.error("Nincs betÃ¶ltÃ¶tt dokumentum az index lÃ©trehozÃ¡sÃ¡hoz!")
    else:
        with st.spinner("Index lÃ©trehozÃ¡sa..."):
            vectorstore = FAISS.from_documents(all_docs, embedding)
            vectorstore.save_local(faiss_index_path)
            st.success("Index lÃ©trehozva Ã©s mentve!")

def get_answer(query):
    if len(query.split()) > 10:
        return "KÃ©rlek, max 10 szÃ³bÃ³l Ã¡llÃ³ kÃ©rdÃ©st tegyÃ©l fel."
    
    if vectorstore is None:
        return "A tudÃ¡sbÃ¡zis jelenleg nem elÃ©rhetÅ‘."
    
    retriever = vectorstore.as_retriever(search_kwargs={"k": 5})
    relevant_docs = retriever.get_relevant_documents(query)
    
    if not relevant_docs:
        return "Sajnos ebben nem tudok segÃ­teni."
    
    short_query = "VÃ¡laszolj a kÃ©rdÃ©sre rÃ¶viden, max 3 mondatban: " + query
    chain = load_qa_chain(ChatOpenAI(temperature=0, max_tokens=150), chain_type="stuff")
    result = chain.run(input_documents=relevant_docs, question=short_query)
    return result

# ğŸŒ Streamlit felÃ¼let
st.title("ğŸ¡ Ingatlan Chatbot Demo (limitÃ¡lt)")
question = st.text_input("âœï¸ Tedd fel a kÃ©rdÃ©sed (max 10 szÃ³):")

if question:
    response = get_answer(question)
    st.write("\n\n**VÃ¡lasz:**")
    st.write(response)
