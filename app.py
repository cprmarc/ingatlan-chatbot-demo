import streamlit as st
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.text_splitter import CharacterTextSplitter
from langchain.chains import RetrievalQA
from langchain.chat_models import ChatOpenAI
from langchain.docstore.document import Document
from langchain.chains.question_answering import load_qa_chain

import os
import glob
import requests
from bs4 import BeautifulSoup
import pickle

# üìö Tud√°sanyag bet√∂lt√©se helyi f√°jlokb√≥l
document_dir = "tudasanyagok"  # ide dobhatod a .txt f√°jlokat
all_docs = []

text_splitter = CharacterTextSplitter(separator="\n", chunk_size=300, chunk_overlap=30)

for filepath in glob.glob(os.path.join(document_dir, "*.txt")):
    with open(filepath, "r", encoding="utf-8") as file:
        content = file.read()
        chunks = text_splitter.split_text(content)
        all_docs.extend([Document(page_content=chunk) for chunk in chunks])

# üåç Online anyagok URL-jei
url_list = [
    # Ide √≠rd a hasznos cikkek URL-jeit
    "https://www.penzcentrum.hu/otthon/ingatlanvasarlas-tanacsok-2025-01-01",
]

def scrape_url(url):
    try:
        response = requests.get(url)
        soup = BeautifulSoup(response.text, "html.parser")
        paragraphs = soup.find_all("p")
        content = "\n".join(p.get_text() for p in paragraphs)
        return content
    except Exception:
        return ""

for url in url_list:
    text = scrape_url(url)
    if text:
        chunks = text_splitter.split_text(text)
        all_docs.extend([Document(page_content=chunk) for chunk in chunks])

# üìÅ Embedding cache el√©r√©si √∫t
cache_path = "faiss_vectorstore.pkl"

# ‚öñÔ∏è Embedding l√©trehoz√°sa √©s indexel√©s (cache-elve)
embedding = OpenAIEmbeddings()

if os.path.exists(cache_path):
    with open(cache_path, "rb") as f:
        vectorstore = pickle.load(f)
else:
    vectorstore = FAISS.from_documents(all_docs, embedding)
    with open(cache_path, "wb") as f:
        pickle.dump(vectorstore, f)

# ‚úâÔ∏è K√©rd√©s-v√°lasz rendszer: max 5 relev√°ns dokumentum, v√°lasz max 3 mondat
def get_answer(query):
    if len(query.split()) > 10:
        return "K√©rlek, max 10 sz√≥b√≥l √°ll√≥ k√©rd√©st tegy√©l fel."

    retriever = vectorstore.as_retriever(search_kwargs={"k": 5})
    relevant_docs = retriever.get_relevant_documents(query)
    if not relevant_docs:
        return "Sajnos ebben nem tudok seg√≠teni."

    # Prompt, hogy max 3 mondat legyen a v√°lasz
    prompt = (
        "V√°laszolj a k√©rd√©sre legfeljebb h√°rom mondatban, egyszer≈±en √©s t√∂m√∂ren.\n\n"
        "K√©rd√©s: {question}\n"
        "V√°lasz:"
    )

    chain = load_qa_chain(ChatOpenAI(temperature=0), chain_type="stuff")
    result = chain.run(input_documents=relevant_docs, question=prompt.format(question=query))
    return result

# üåê Streamlit fel√ºlet
st.title("üè° Ingatlan Chatbot Demo (limit√°lt)")

question = st.text_input("‚úçÔ∏è Tedd fel a k√©rd√©sed (max 10 sz√≥):")

if question:
    response = get_answer(question)
    st.write("\n\n**V√°lasz:**")
    st.write(response)
