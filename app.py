import streamlit as st
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.text_splitter import CharacterTextSplitter
from langchain.docstore.document import Document
from langchain.chains.question_answering import load_qa_chain
from langchain.chat_models import ChatOpenAI

import os
import requests
from bs4 import BeautifulSoup
import pickle

# Sz√∂vegt√∂rdel≈ë be√°ll√≠t√°sa
text_splitter = CharacterTextSplitter(separator="\n", chunk_size=300, chunk_overlap=30)

all_docs = []

# üìö Helyi f√°jlok bet√∂lt√©se (jelenleg kikommentelve, mert nincs f√°jl)
# document_dir = "tudasanyagok"  # ide dobhatod a .txt f√°jlokat
# import glob
# for filepath in glob.glob(os.path.join(document_dir, "*.txt")):
#     with open(filepath, "r", encoding="utf-8") as file:
#         content = file.read()
#         chunks = text_splitter.split_text(content)
#         all_docs.extend([Document(page_content=chunk) for chunk in chunks])

# üåç Online anyagok URL-jei - ide m√°sold be a cikkek URL-jeit
url_list = [
    "https://tudastar.ingatlan.com/tippek/az-ingatlanvasarlas-menete/",
    "https://tudastar.ingatlan.com/tippek/tulajdonjog-fenntartashoz-kapcsolodo-vevoi-jog/",
    "https://tudastar.ingatlan.com/tippek/birtokbaadasi-jegyzokonyv-mire-valo-miert-jo-hogyan-toltsd-ki/",
   "https://bankmonitor.hu/lakashitel-igenyles/...",
]

def scrape_url(url):
    try:
        response = requests.get(url)
        soup = BeautifulSoup(response.text, "html.parser")
        paragraphs = soup.find_all("p")
        content = "\n".join(p.get_text() for p in paragraphs)
        return content
    except Exception:
        return ""

# Online cikkek feldolgoz√°sa, sz√∂veg feldarabol√°sa
for url in url_list:
    text = scrape_url(url)
    if text:
        chunks = text_splitter.split_text(text)
        all_docs.extend([Document(page_content=chunk) for chunk in chunks])

# üìÅ Cache f√°jl el√©r√©si √∫tja
cache_path = "faiss_vectorstore.pkl"

# Embedding p√©ld√°ny
embedding = OpenAIEmbeddings()

# Vektor adatb√°zis bet√∂lt√©se vagy l√©trehoz√°sa
vectorstore = None
if os.path.exists(cache_path):
    try:
        with open(cache_path, "rb") as f:
            vectorstore = pickle.load(f)
    except Exception:
        st.warning("Cache bet√∂lt√©se sikertelen, √∫jragener√°lom az indexet.")
        vectorstore = None

if vectorstore is None:
    if not all_docs:
        st.error("Nincs bet√∂lt√∂tt dokumentum az index l√©trehoz√°s√°hoz! K√©rlek adj hozz√° adatokat.")
    else:
        vectorstore = FAISS.from_documents(all_docs, embedding)
        with open(cache_path, "wb") as f:
            pickle.dump(vectorstore, f)

def get_answer(query):
    if len(query.split()) > 10:
        return "K√©rlek, max 10 sz√≥b√≥l √°ll√≥ k√©rd√©st tegy√©l fel."

    if vectorstore is None:
        return "A tud√°sb√°zis jelenleg nem el√©rhet≈ë."

    retriever = vectorstore.as_retriever(search_kwargs={"k": 5})
    relevant_docs = retriever.get_relevant_documents(query)
    if not relevant_docs:
        return
