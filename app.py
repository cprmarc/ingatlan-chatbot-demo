import streamlit as st
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.text_splitter import CharacterTextSplitter
from langchain.docstore.document import Document
from langchain.chains.question_answering import load_qa_chain
from langchain.chat_models import ChatOpenAI

import os
import requests
from bs4 import BeautifulSoup
import pickle

# SzÃ¶vegtÃ¶rdelÅ‘ beÃ¡llÃ­tÃ¡sa
text_splitter = CharacterTextSplitter(separator="\n", chunk_size=300, chunk_overlap=30)

all_docs = []

# ğŸ“š Helyi fÃ¡jlok betÃ¶ltÃ©se (jelenleg kikommentelve, mert nincs fÃ¡jl)
# document_dir = "tudasanyagok"  # ide dobhatod a .txt fÃ¡jlokat
# import glob
# for filepath in glob.glob(os.path.join(document_dir, "*.txt")):
#     with open(filepath, "r", encoding="utf-8") as file:
#         content = file.read()
#         chunks = text_splitter.split_text(content)
#         all_docs.extend([Document(page_content=chunk) for chunk in chunks])

# ğŸŒ Online anyagok URL-jei - ide mÃ¡sold be a cikkek URL-jeit
url_list = [
    "https://tudastar.ingatlan.com/tippek/az-ingatlanvasarlas-menete/",
    "https://tudastar.ingatlan.com/tippek/tulajdonjog-fenntartashoz-kapcsolodo-vevoi-jog/",
    "https://tudastar.ingatlan.com/tippek/birtokbaadasi-jegyzokonyv-mire-valo-miert-jo-hogyan-toltsd-ki/",
    "https://bankmonitor.hu/lakashitel-igenyles",
]

def scrape_url(url):
    try:
        response = requests.get(url)
        soup = BeautifulSoup(response.text, "html.parser")
        paragraphs = soup.find_all("p")
        content = "\n".join(p.get_text() for p in paragraphs)
        return content
    except Exception as e:
        st.warning(f"Nem sikerÃ¼lt lekÃ©rni az URL-t: {url}. Hiba: {e}")
        return ""

# Online cikkek feldolgozÃ¡sa, szÃ¶veg feldarabolÃ¡sa
for url in url_list:
    text = scrape_url(url)
    if text:
        chunks = text_splitter.split_text(text)
        all_docs.extend([Document(page_content=chunk) for chunk in chunks])

# ğŸ“ Cache fÃ¡jl elÃ©rÃ©si Ãºtja
cache_path = "faiss_vectorstore.pkl"

# Embedding pÃ©ldÃ¡ny
embedding = OpenAIEmbeddings()

vectorstore = None

# Vektor adatbÃ¡zis betÃ¶ltÃ©se vagy lÃ©trehozÃ¡sa
if os.path.exists(cache_path):
    try:
        with open(cache_path, "rb") as f:
            vectorstore = pickle.load(f)
    except Exception as e:
        st.warning(f"Cache betÃ¶ltÃ©se sikertelen, ÃºjragenerÃ¡lom az indexet. Hiba: {e}")
        vectorstore = None

if vectorstore is None:
    if not all_docs:
        st.error("Nincs betÃ¶ltÃ¶tt dokumentum az index lÃ©trehozÃ¡sÃ¡hoz! KÃ©rlek adj hozzÃ¡ adatokat.")
    else:
        vectorstore = FAISS.from_documents(all_docs, embedding)
        with open(cache_path, "wb") as f:
            pickle.dump(vectorstore, f)

def get_answer(query):
    if len(query.split()) > 10:
        return "KÃ©rlek, max 10 szÃ³bÃ³l Ã¡llÃ³ kÃ©rdÃ©st tegyÃ©l fel."

    if vectorstore is None:
        return "A tudÃ¡sbÃ¡zis jelenleg nem elÃ©rhetÅ‘."

    retriever = vectorstore.as_retriever(search_kwargs={"k": 5})
    relevant_docs = retriever.get_relevant_documents(query)
    if not relevant_docs:
        return "Sajnos ebben nem tudok segÃ­teni."

    short_query = "VÃ¡laszolj a kÃ©rdÃ©sre rÃ¶viden, max 3 mondatban: " + query

    chain = load_qa_chain(ChatOpenAI(temperature=0, max_tokens=150), chain_type="stuff")
    result = chain.run(input_documents=relevant_docs, question=short_query)
    return result

# ğŸŒ Streamlit felÃ¼let
st.title("ğŸ¡ Ingatlan Chatbot Demo (limitÃ¡lt)")

question = st.text_input("âœï¸ Tedd fel a kÃ©rdÃ©sed (max 10 szÃ³):")

if question:
    response = get_answer(question)
    st.write("\n\n**VÃ¡lasz:**")
    st.write(response)
